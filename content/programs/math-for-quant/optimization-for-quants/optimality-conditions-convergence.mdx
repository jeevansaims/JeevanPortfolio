# Optimality Conditions and Convergence Intuition

## 1. Why Optimality Conditions Matter

### How We Know a Solution Is Optimal

Running an optimization algorithm produces a candidate solution. But how do you know it's actually optimal?

**The problem:** Algorithms iterate until some stopping criterion is met (small gradient, small step, max iterations). But stopping doesn't guarantee optimality.

**Optimality conditions** provide mathematical tests. If a candidate passes these tests, you have guarantees about its quality.

**First-order conditions:** Necessary conditions that any local optimum must satisfy.

**Second-order conditions:** Additional conditions that distinguish minima from maxima and saddles.

### Distinguishing Candidates from True Optima

**Not every stationary point is optimal:**

- A point with zero gradient might be a minimum, maximum, or saddle
- Gradient descent can stop at saddles (in high dimensions, this is common)
- Numerical tolerances mean "zero gradient" is really "small gradient"

**Optimality conditions help you:**

- Verify that a candidate is truly a local minimum
- Identify when you've found a saddle point
- Understand why the algorithm stopped where it did

### Why Quants Care About Guarantees and Stability

In production systems, you need confidence in your results:

**Regulatory requirements:** Risk calculations must be defensible. "The algorithm stopped" isn't sufficient justification.

**Financial decisions:** Billions of dollars depend on portfolio optimization. You need to know you've found the right answer.

**Model validation:** When calibrating models, you must verify the optimization succeeded. Poor calibration leads to mispriced risk.

**Debugging:** When results look wrong, optimality conditions help diagnose whether the problem is in the formulation, the data, or the algorithm.

---

## 2. First-Order Optimality Conditions

### Vanishing Gradient as a Necessary Condition

For unconstrained optimization of a differentiable function $f$:

**First-order necessary condition:** If $x^*$ is a local minimum, then:

$$
\nabla f(x^*) = 0
$$

**Why necessary:** If the gradient weren't zero, you could decrease $f$ by moving in the direction $-\nabla f$. So any local minimum must have zero gradient.

**Interpretation:** At an optimum, there's no direction of immediate improvement. You're at a "flat spot" in the landscape.

### Stationary Points

A **stationary point** is any point where $\nabla f(x) = 0$.

**Types of stationary points:**

- Local minimum: Can't improve by small movements
- Local maximum: Can't worsen by small movements
- Saddle point: Can improve in some directions, worsen in others

**Key insight:** Zero gradient tells you you're at a stationary point, but not which type. You need more information.

### Why First-Order Conditions Are Not Sufficient on Their Own

**The problem:** $\nabla f(x^*) = 0$ is necessary but not sufficient for a minimum.

**Example:** $f(x) = x^3$ at $x = 0$.

The gradient is $f'(x) = 3x^2$, which equals zero at $x = 0$. But $x = 0$ is an inflection point, not a minimum or maximum.

**Example:** $f(x) = -x^2$ at $x = 0$.

The gradient is zero at $x = 0$, but this is a maximum, not a minimum.

**Example:** $f(x,y) = x^2 - y^2$ at $(0,0)$.

The gradient is zero, but this is a saddle point.

**Conclusion:** First-order conditions identify candidates. Second-order conditions verify them.

---

## 3. Second-Order Optimality Conditions

### Role of the Hessian

The Hessian matrix $H(x)$ contains second derivatives and captures curvature. It tells you the shape of the function near a stationary point.

**Second-order necessary condition:** If $x^*$ is a local minimum, then:

$$
H(x^*) \succeq 0 \quad \text{(positive semi-definite)}
$$

All eigenvalues of the Hessian are non-negative.

**Second-order sufficient condition:** If $\nabla f(x^*) = 0$ and:

$$
H(x^*) \succ 0 \quad \text{(positive definite)}
$$

Then $x^*$ is a strict local minimum.

### Positive Definite Hessian and Local Minima

**Positive definite** means all eigenvalues are strictly positive. The function curves upward in every direction.

**Geometric interpretation:** You're at the bottom of a bowl. Any direction you move increases the function value.

**Why it guarantees a minimum:** The second-order Taylor expansion is:

$$
f(x^* + d) \approx f(x^*) + \frac{1}{2} d^T H(x^*) d
$$

Since $\nabla f(x^*) = 0$ and $H \succ 0$, we have $d^T H d > 0$ for any direction $d$. So $f(x^* + d) > f(x^*)$.

### Indefinite Hessian and Saddle Points

**Indefinite** means some eigenvalues are positive and some are negative.

**Geometric interpretation:** The function curves upward in some directions but downward in others. You're at a saddle.

**At a saddle point:**

- Moving along positive-eigenvalue directions increases $f$
- Moving along negative-eigenvalue directions decreases $f$
- This is not an optimum in any sense

**Detection:** If $\nabla f(x^*) = 0$ and $H(x^*)$ has both positive and negative eigenvalues, $x^*$ is a saddle point.

### Why Curvature Completes the Picture

**First-order:** Tells you if you're at a flat spot (zero gradient)

**Second-order:** Tells you the shape of that flat spot

**Together:** They fully characterize local behavior for smooth functions

**The classification:**

- $\nabla f = 0$ and $H \succ 0$: Local minimum (confirmed)
- $\nabla f = 0$ and $H \prec 0$: Local maximum
- $\nabla f = 0$ and $H$ indefinite: Saddle point
- $\nabla f = 0$ and $H$ semi-definite: Inconclusive, need higher-order analysis

---

## 4. Local vs Global Optimality

### Multiple Optima in Non-Convex Problems

**Non-convex functions** can have multiple local minima:

**Example:** $f(x) = x^4 - 2x^2$ has local minima at $x = \pm 1$ and a local maximum at $x = 0$.

**The challenge:** First and second-order conditions only identify local optima. They can't tell you if a better solution exists elsewhere.

**In practice:** Algorithms find whichever local minimum they converge to. Different starting points may yield different solutions.

### How Convexity Simplifies Optimality

For **convex functions**, the picture simplifies dramatically:

**Key theorem:** For convex $f$, every local minimum is a global minimum.

**Implication:** If you find a point satisfying $\nabla f(x^*) = 0$ (and the domain is convex), you've found the global optimum. No need for second-order checks, no worry about other local minima.

**Why this works:** Convexity means the function curves upward everywhere. There can't be multiple valleys, only one bowl.

### Practical Implications for Quant Models

**Portfolio optimization:** Variance is convex, so mean-variance optimization has a unique solution. The first-order conditions are sufficient.

**Model calibration:** Many calibration objectives are non-convex. Multiple local minima exist. Different starting points matter.

**Risk models:** Convex risk measures (like variance, CVaR) lead to well-behaved optimization. Non-convex measures (like VaR) create complications.

**Strategy:** When possible, formulate problems as convex. When not possible, use multiple starting points and compare solutions.

---

## 5. Convergence of Optimization Algorithms

### What Convergence Means in Practice

**Theoretical convergence:** The sequence $x_k$ approaches a limit $x^*$ as $k \to \infty$.

**Practical convergence:** After finite iterations, you're "close enough" to a solution.

**Measures of closeness:**

- $\|x_k - x^*\|$ small (parameter error)
- $|f(x_k) - f(x^*)|$ small (objective error)
- $\|\nabla f(x_k)\|$ small (optimality error)

In practice, you never reach the exact optimum. You stop when error is below tolerance.

### Convergence to Stationary Points

**What gradient descent guarantees:** For smooth functions bounded below, gradient descent converges to a stationary point (where $\nabla f = 0$).

**What it doesn't guarantee:**

- That the stationary point is a minimum (could be saddle)
- That it's a global minimum (could be local)
- How fast you get there

**For convex functions:** Convergence to stationary point equals convergence to global minimum. The guarantees are stronger.

**For non-convex functions:** You might converge to saddles or local minima. Multiple runs with different starting points help.

### Dependence on Problem Structure

**Convergence depends on:**

**Smoothness:** How fast can the gradient change? Smoother functions allow larger steps.

**Convexity:** Convex functions have no bad local minima. Convergence guarantees are stronger.

**Conditioning:** How different are curvatures in different directions? Ill-conditioned problems converge slowly.

**Boundedness:** Is there a minimum? Unbounded problems may not converge.

**Problem structure determines both whether algorithms converge and how fast.**

---

## 6. Convergence Speed (Intuition)

### Linear vs Superlinear Convergence (Conceptual)

**Linear convergence:** Error decreases by a constant factor each iteration.

$$
\|x_{k+1} - x^*\| \leq c \|x_k - x^*\| \quad \text{for some } c < 1
$$

If $c = 0.9$, error shrinks by 10% per iteration. After 100 iterations, error is $0.9^{100} \approx 0.00003$ of original.

**Superlinear convergence:** Error factor improves each iteration.

$$
\frac{\|x_{k+1} - x^*\|}{\|x_k - x^*\|} \to 0
$$

Each iteration becomes more effective than the last.

**Quadratic convergence:** Error squares each iteration.

$$
\|x_{k+1} - x^*\| \leq C \|x_k - x^*\|^2
$$

If error is 0.1, next error is 0.01, then 0.0001. The number of correct digits roughly doubles per iteration.

### Why Curvature Affects Speed

**The condition number** $\kappa = \lambda_{max}/\lambda_{min}$ determines convergence speed for gradient descent:

**Well-conditioned ($\kappa$ small):** Similar curvature in all directions. Gradient descent converges quickly, taking efficient steps.

**Ill-conditioned ($\kappa$ large):** Very different curvatures. Gradient descent zigzags, converging slowly.

**Convergence rate for gradient descent:** Roughly $(1 - 1/\kappa)$ per iteration. If $\kappa = 100$, error shrinks by only 1% per iteration.

**In finance:** Highly correlated assets create ill-conditioned covariance matrices. Portfolio optimization slows down.

### Why Newton-Type Methods Converge Faster

**Newton's method** uses the Hessian to account for curvature:

$$
x_{k+1} = x_k - H(x_k)^{-1} \nabla f(x_k)
$$

**Near a minimum:** Newton achieves quadratic convergence. The number of correct digits doubles each iteration.

**Why faster:** Newton adapts step size to curvature in each direction. It doesn't suffer from ill-conditioning the way gradient descent does.

**The trade-off:**

- Gradient descent: Cheap iterations, many needed
- Newton: Expensive iterations (Hessian computation), few needed

**Quasi-Newton methods** (like BFGS) approximate the Hessian, getting some of Newton's speed without the full cost.

---

## 7. Summary

### Key Concepts

**First-order conditions** ($\nabla f = 0$) identify stationary points but don't distinguish minima from saddles.

**Second-order conditions** (Hessian positive definite) confirm local minima.

**Convexity** ensures local minima are global, simplifying optimization.

**Convergence** depends on problem structure: smoothness, convexity, and conditioning.

**Convergence speed** ranges from linear (gradient descent) to quadratic (Newton) depending on the method and problem.

### Key Formulas

**First-order necessary condition:**

$$
\nabla f(x^*) = 0
$$

**Second-order sufficient condition:**

$$
\nabla f(x^*) = 0 \text{ and } H(x^*) \succ 0 \implies x^* \text{ is a local minimum}
$$

**Linear convergence:**

$$
\|x_{k+1} - x^*\| \leq c \|x_k - x^*\|
$$

### Key Insights for Quants

1. **Zero gradient is necessary but not sufficient** for optimality
2. **Positive definite Hessian confirms** a local minimum
3. **Convexity eliminates local minima concerns**, making optimization reliable
4. **Conditioning affects convergence speed** dramatically
5. **Newton methods trade iteration cost for convergence speed**

---

## 8. What's Next?

We've covered unconstrained optimization thoroughly. But real financial problems have constraints: budgets must balance, positions have limits, risk must be controlled.

**Next lesson: Constrained Optimization**

You'll learn:

- How constraints change the optimization problem
- Equality and inequality constraints
- The feasible region and its geometry
- Active vs inactive constraints
