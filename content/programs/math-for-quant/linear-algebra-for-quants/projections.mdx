# Projections

## 1. The Idea of Projection

### Projecting a Vector onto a Direction or Subspace

A **projection** decomposes a vector into two parts:

- The part that lies **within** a subspace (the "explained" component)
- The part that is **orthogonal** to the subspace (the "residual" or "unexplained" component)

$$
\mathbf{b} = \mathbf{b}_{\text{proj}} + \mathbf{b}_{\text{residual}}
$$

where $\mathbf{b}_{\text{proj}} \perp \mathbf{b}_{\text{residual}}$.

### Why Projections Matter: Separating Signal from Noise

In quantitative finance, we constantly need to separate:

- **Systematic risk** (explained by factors) from **idiosyncratic risk** (specific to the asset)
- **Predictable components** of returns from **noise**
- **Hedgeable exposure** from **unhedgeable residual**

Projections provide the mathematical machinery for these decompositions.

### Geometric Intuition

Imagine shining a light directly above a vector $\mathbf{b}$ onto a line or plane. The **shadow** is the projection—the closest point in the subspace to $\mathbf{b}$.

**Key insight:** The projection minimizes the distance from $\mathbf{b}$ to the subspace. This is why projections are central to **least squares** and **best approximation** problems.

---

## 2. Projection onto a Line

### Formula for Projection onto a Single Vector

The projection of vector $\mathbf{b}$ onto the direction of vector $\mathbf{a}$ is:

$$
\text{proj}_{\mathbf{a}} \mathbf{b} = \frac{\mathbf{a} \cdot \mathbf{b}}{\mathbf{a} \cdot \mathbf{a}} \mathbf{a} = \frac{\mathbf{a}^T \mathbf{b}}{\mathbf{a}^T \mathbf{a}} \mathbf{a}
$$

**Components of the formula:**

- $\mathbf{a}^T \mathbf{b}$ = dot product (measures alignment)
- $\mathbf{a}^T \mathbf{a} = \|\mathbf{a}\|^2$ = squared length of $\mathbf{a}$
- The scalar $c = \frac{\mathbf{a}^T \mathbf{b}}{\mathbf{a}^T \mathbf{a}}$ tells us "how much" of $\mathbf{a}$ is in $\mathbf{b}$

### Decomposing a Vector into Parallel and Orthogonal Parts

Any vector $\mathbf{b}$ can be written as:

$$
\mathbf{b} = \mathbf{b}_{\parallel} + \mathbf{b}_{\perp}
$$

where:

- $\mathbf{b}_{\parallel} = \text{proj}_{\mathbf{a}} \mathbf{b}$ (parallel to $\mathbf{a}$)
- $\mathbf{b}_{\perp} = \mathbf{b} - \text{proj}_{\mathbf{a}} \mathbf{b}$ (orthogonal to $\mathbf{a}$)

### Interpretation: Best Approximation in That Direction

The projection $\text{proj}_{\mathbf{a}} \mathbf{b}$ is the **closest point** to $\mathbf{b}$ that lies on the line spanned by $\mathbf{a}$.

**Optimization viewpoint:** Among all scalar multiples $c\mathbf{a}$, the projection minimizes:

$$
\|\mathbf{b} - c\mathbf{a}\|^2
$$

### Orthogonality of the Residual

**Critical property:** The residual is always orthogonal to the projection direction:

$$
\mathbf{a}^T (\mathbf{b} - \text{proj}_{\mathbf{a}} \mathbf{b}) = 0
$$

This orthogonality is what makes the projection the "best" approximation.

### Example: Projection onto a Line

Project $\mathbf{b} = \begin{pmatrix} 3 \\ 4 \end{pmatrix}$ onto $\mathbf{a} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}$.

**Step 1:** Compute dot products

$$
\mathbf{a}^T \mathbf{b} = 2(3) + 1(4) = 10
$$

$$
\mathbf{a}^T \mathbf{a} = 2^2 + 1^2 = 5
$$

**Step 2:** Compute projection

$$
\text{proj}_{\mathbf{a}} \mathbf{b} = \frac{10}{5} \begin{pmatrix} 2 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 2 \end{pmatrix}
$$

**Step 3:** Compute residual

$$
\mathbf{b}_{\perp} = \begin{pmatrix} 3 \\ 4 \end{pmatrix} - \begin{pmatrix} 4 \\ 2 \end{pmatrix} = \begin{pmatrix} -1 \\ 2 \end{pmatrix}
$$

**Verify orthogonality:** $\mathbf{a}^T \mathbf{b}_{\perp} = 2(-1) + 1(2) = 0$ ✓

---

## 3. Projection onto a Subspace

### Generalizing from a Line to Any Subspace

When projecting onto a subspace spanned by multiple vectors (columns of matrix $A$), we need to find the point in $\text{Col}(A)$ closest to $\mathbf{b}$.

We seek $\hat{\mathbf{b}} = A\mathbf{x}$ for some $\mathbf{x}$ such that $\mathbf{b} - A\mathbf{x}$ is orthogonal to every column of $A$.

### Using an Orthonormal Basis

If columns of $A$ are **orthonormal** (orthogonal and unit length), projection simplifies dramatically:

$$
\text{proj}_{\text{Col}(A)} \mathbf{b} = AA^T \mathbf{b}
$$

Each component is just a dot product with the corresponding basis vector.

### The General Projection Formula

For any matrix $A$ with linearly independent columns:

$$
P = A(A^T A)^{-1} A^T
$$

The projection of $\mathbf{b}$ onto $\text{Col}(A)$ is:

$$
\hat{\mathbf{b}} = P\mathbf{b} = A(A^T A)^{-1} A^T \mathbf{b}
$$

**Why this formula works:**

1. We need $\mathbf{b} - A\mathbf{x}$ orthogonal to column space of $A$
2. This means $A^T(\mathbf{b} - A\mathbf{x}) = \mathbf{0}$
3. Solving: $A^T A \mathbf{x} = A^T \mathbf{b}$
4. Therefore: $\mathbf{x} = (A^T A)^{-1} A^T \mathbf{b}$
5. And: $\hat{\mathbf{b}} = A\mathbf{x} = A(A^T A)^{-1} A^T \mathbf{b}$

### Why Orthonormalization Simplifies Everything

If $Q$ has orthonormal columns ($Q^T Q = I$), then:

$$
P = Q(Q^T Q)^{-1} Q^T = QQ^T
$$

No matrix inverse needed! This is why **Gram-Schmidt orthogonalization** and **QR decomposition** are so valuable.

---

## 4. Properties of Projection Matrices

### Symmetric Matrices

Every projection matrix onto a subspace is **symmetric**:

$$
P^T = P
$$

**Proof:** $P^T = (A(A^T A)^{-1} A^T)^T = A((A^T A)^{-1})^T A^T = A(A^T A)^{-1} A^T = P$

### Idempotence: $P^2 = P$

Projecting twice gives the same result as projecting once:

$$
P^2 = P
$$

**Intuition:** Once a vector is in the subspace, projecting again doesn't move it.

**Proof:** $P^2 = A(A^T A)^{-1} A^T A(A^T A)^{-1} A^T = A(A^T A)^{-1} A^T = P$

### What These Properties Tell You

Given an unknown matrix $M$, you can identify it as a projection matrix if:

1. $M^T = M$ (symmetric)
2. $M^2 = M$ (idempotent)

These two conditions are **necessary and sufficient**.

### Eigenvalues of Projection Matrices

Projection matrices have only two possible eigenvalues: **0** and **1**.

- Eigenvalue 1: eigenvectors in the projection subspace (unchanged by $P$)
- Eigenvalue 0: eigenvectors orthogonal to the subspace (sent to zero by $P$)

**Rank of $P$:** equals the dimension of the subspace = number of 1-eigenvalues.

---

## 5. Least Squares as a Projection Problem

### Solving $A\mathbf{x} \approx \mathbf{b}$ via Projection

When the system $A\mathbf{x} = \mathbf{b}$ has no exact solution (more equations than unknowns, or $\mathbf{b} \notin \text{Col}(A)$), we find the **best approximation**.

**Least squares solution:** Find $\hat{\mathbf{x}}$ that minimizes:

$$
\|A\mathbf{x} - \mathbf{b}\|^2
$$

### The Normal Equations

The minimizer satisfies the **normal equations**:

$$
A^T A \hat{\mathbf{x}} = A^T \mathbf{b}
$$

**Solution:**

$$
\hat{\mathbf{x}} = (A^T A)^{-1} A^T \mathbf{b}
$$

### Geometric Meaning

The least squares solution projects $\mathbf{b}$ onto $\text{Col}(A)$:

$$
A\hat{\mathbf{x}} = P\mathbf{b} = \text{proj}_{\text{Col}(A)} \mathbf{b}
$$

The residual $\mathbf{b} - A\hat{\mathbf{x}}$ is orthogonal to every column of $A$.

### Why Least Squares Is a Projection

Least squares isn't arbitrary curve fitting—it's the **unique** solution where:

1. The fitted values lie in the column space of $A$
2. The residuals are orthogonal to that column space

This orthogonality principle is what makes least squares mathematically special.

### Sets the Stage for Regression

In regression $\mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\epsilon}$:

- $X$ = design matrix (predictors)
- $\mathbf{y}$ = response (returns)
- $\hat{\boldsymbol{\beta}} = (X^T X)^{-1} X^T \mathbf{y}$ = OLS estimates
- $\hat{\mathbf{y}} = X\hat{\boldsymbol{\beta}}$ = fitted values (projection of $\mathbf{y}$ onto column space of $X$)
- $\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}}$ = residuals (orthogonal to predictors)

---

## 6. Orthogonal Complements

### Residual Space as the Orthogonal Complement

For any subspace $S$, the **orthogonal complement** $S^{\perp}$ consists of all vectors perpendicular to every vector in $S$:

$$
S^{\perp} = \{\mathbf{v} : \mathbf{v}^T \mathbf{s} = 0 \text{ for all } \mathbf{s} \in S\}
$$

### Decomposition of Space

Every vector $\mathbf{b}$ in $\mathbb{R}^n$ can be uniquely written as:

$$
\mathbf{b} = \mathbf{b}_S + \mathbf{b}_{S^{\perp}}
$$

where $\mathbf{b}_S \in S$ and $\mathbf{b}_{S^{\perp}} \in S^{\perp}$.

**Dimensions add up:**

$$
\dim(S) + \dim(S^{\perp}) = n
$$

### Null Space and Orthogonality Revisited

Recall from the four fundamental subspaces:

- $\text{Row}(A)^{\perp} = \text{Null}(A)$
- $\text{Col}(A)^{\perp} = \text{Null}(A^T)$

Projections naturally connect to null spaces through orthogonality.

### Why Splitting Space Is Powerful

This decomposition lets us:

1. **Separate explained from unexplained variance** (regression)
2. **Isolate factor exposure from idiosyncratic risk** (factor models)
3. **Identify hedgeable vs. unhedgeable components** (risk management)
4. **Orthogonalize correlated signals** (signal processing)

### Complementary Projection

If $P$ projects onto subspace $S$, then:

$$
I - P
$$

projects onto $S^{\perp}$.

**Check:** $(I - P)^2 = I - 2P + P^2 = I - 2P + P = I - P$ ✓

---

## 7. Quant Applications

### Linear Regression: Predicting Returns Using Factor Exposures

In factor-based return prediction:

$$
r_i = \alpha_i + \beta_{i,1} f_1 + \beta_{i,2} f_2 + \cdots + \beta_{i,k} f_k + \epsilon_i
$$

The factor exposures $\beta$ are estimated by projecting returns onto factor space:

$$
\hat{\boldsymbol{\beta}} = (F^T F)^{-1} F^T \mathbf{r}
$$

where $F$ is the matrix of factor returns.

### Projecting Returns onto Factor Space to Extract Betas

**Systematic component:** $\hat{\mathbf{r}}_{\text{systematic}} = F\hat{\boldsymbol{\beta}}$ (projection onto factor space)

**Idiosyncratic component:** $\hat{\boldsymbol{\epsilon}} = \mathbf{r} - \hat{\mathbf{r}}_{\text{systematic}}$ (residual, orthogonal to factors)

The systematic component captures risk explained by known factors. The residual is stock-specific risk.

### Orthogonalizing Signals to Remove Redundancy

If you have multiple trading signals that are correlated, projections help **orthogonalize** them:

1. Keep signal 1 as-is
2. Replace signal 2 with: signal 2 minus its projection onto signal 1
3. The new signal 2 captures information **not already in** signal 1

This is the essence of **Gram-Schmidt orthogonalization** applied to trading signals.

### Residuals as Idiosyncratic Returns in Factor Models

In a factor model:

$$
\mathbf{r} = B\mathbf{f} + \boldsymbol{\epsilon}
$$

- $B\mathbf{f}$ = systematic return (projection onto factor space)
- $\boldsymbol{\epsilon}$ = idiosyncratic return (orthogonal to factors)

**Portfolio implication:** Idiosyncratic risk can be diversified away. Systematic risk cannot.

### Hedging via Projection

To hedge asset returns $\mathbf{r}_{\text{asset}}$ using hedge instruments with returns $H$:

$$
\mathbf{w}_{\text{hedge}} = (H^T H)^{-1} H^T \mathbf{r}_{\text{asset}}
$$

The hedged position has return:

$$
\mathbf{r}_{\text{hedged}} = \mathbf{r}_{\text{asset}} - H\mathbf{w}_{\text{hedge}}
$$

This residual is orthogonal to the hedge instruments—we've removed the hedgeable component.

---

## 8. Worked Examples

### Example 1: Project onto a Subspace

Project $\mathbf{b} = \begin{pmatrix} 1 \\ 2 \\ 2 \end{pmatrix}$ onto the plane spanned by $\mathbf{a}_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}$ and $\mathbf{a}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}$.

**Solution:**

$$
A = \begin{pmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{pmatrix}
$$

Since columns are orthonormal:

$$
P = AA^T = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix}
$$

$$
\hat{\mathbf{b}} = P\mathbf{b} = \begin{pmatrix} 1 \\ 2 \\ 0 \end{pmatrix}
$$

Residual: $\mathbf{b} - \hat{\mathbf{b}} = \begin{pmatrix} 0 \\ 0 \\ 2 \end{pmatrix}$

### Example 2: Least Squares Line Fitting

Fit a line $y = mx$ to points $(1, 2), (2, 3), (3, 5)$.

**Setup:**

$$
A = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} 2 \\ 3 \\ 5 \end{pmatrix}
$$

**Normal equation:**

$$
A^T A = 1 + 4 + 9 = 14
$$

$$
A^T \mathbf{b} = 1(2) + 2(3) + 3(5) = 23
$$

$$
\hat{m} = \frac{23}{14} \approx 1.64
$$

### Example 3: Verify Projection Matrix Properties

Given $P = \begin{pmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{pmatrix}$, verify it's a projection matrix.

**Symmetry:** $P^T = P$ ✓

**Idempotence:**

$$
P^2 = \begin{pmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{pmatrix} \begin{pmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{pmatrix} = \begin{pmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{pmatrix} = P
$$

✓ This projects onto the line $y = x$.

---

## 9. Summary

### Key Concepts

**Projection onto a line:**

$$
\text{proj}_{\mathbf{a}} \mathbf{b} = \frac{\mathbf{a}^T \mathbf{b}}{\mathbf{a}^T \mathbf{a}} \mathbf{a}
$$

**Projection onto column space of $A$:**

$$
P = A(A^T A)^{-1} A^T
$$

**Properties of projection matrices:**

- Symmetric: $P^T = P$
- Idempotent: $P^2 = P$
- Eigenvalues: only 0 and 1

**Orthogonal decomposition:**

$$
\mathbf{b} = \text{proj}_S \mathbf{b} + \text{proj}_{S^{\perp}} \mathbf{b}
$$

### Key Results

- Projections give the **closest point** in a subspace
- Residuals are **orthogonal** to the projection subspace
- Least squares is **projection** onto column space
- $I - P$ projects onto the orthogonal complement

### Financial Implications

- **Factor models:** Projection separates systematic from idiosyncratic risk
- **Regression:** OLS coefficients come from projecting returns onto predictors
- **Hedging:** Optimal hedge ratios minimize residual variance via projection
- **Signal orthogonalization:** Remove redundancy by projecting out known information

---

## 10. What's Next?

We've learned to project onto arbitrary subspaces. But some directions in a matrix are special—they are **preserved** by the transformation (only scaled, not rotated).

**Next lesson: Eigenvalues & Eigenvectors**

You'll learn:

- Eigenvectors: directions unchanged by a transformation
- Eigenvalues: scaling factors along these special directions
- Diagonalization and its power
- Principal directions of covariance matrices
- Why eigenvectors dominate variance in PCA
